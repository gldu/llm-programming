from typing import List, Dict, Any
from llm_client import HelloLLMAgent
# --- æ¨¡å— 1: è®°å¿†æ¨¡å— ---
class Memory:
    """
    ä¸€ä¸ªç®€å•çš„çŸ­æœŸè®°å¿†æ¨¡å—ï¼Œç”¨äºå­˜å‚¨æ™ºèƒ½ä½“çš„è¡ŒåŠ¨ä¸åæ€è½¨è¿¹ã€‚
    """
    def __init__(self):
        self.records:List[Dict[str,Any]] = []

    def add_record(self,record_type:str,content:str):
        """å‘è®°å¿†ä¸­å¢åŠ ä¸€æ¡è®°å½•"""
        self.records.append({"record_type":record_type,"content":content})
        print(f"ğŸ“ è®°å¿†å·²æ›´æ–°ï¼Œæ–°å¢ä¸€æ¡ '{record_type}' è®°å½•ã€‚")

    def get_trajectory(self) -> str:
        """
        å°†æ‰€æœ‰è®°å¿†è®°å½•æ ¼å¼åŒ–ä¸ºä¸€ä¸ªè¿è´¯çš„å­—ç¬¦ä¸²æ–‡æœ¬ï¼Œç”¨äºæ„å»ºæç¤ºè¯ã€‚
        """
        trajectory = ""
        for record in self.records:
            if(record["record_type"] == "execution"):
                trajectory += f"--- ä¸Šä¸€è½®å°è¯• (ä»£ç ) ---\n{record['content']}\n\n"
            elif record["record_type"]== "reflection":
                trajectory += f"--- è¯„å®¡å‘˜åé¦ˆ ---\n{record['content']}\n\n"
        return trajectory.strip()
    
    def get_last_execution(self)->str:
        """
        è·å–æœ€è¿‘ä¸€æ¬¡çš„æ‰§è¡Œç»“æœ (ä¾‹å¦‚ï¼Œæœ€æ–°ç”Ÿæˆçš„ä»£ç )ã€‚
        """
        for record in reversed(self.records):
            if record["type"] == "execution":
                return record["content"]
        return None

# --- æ¨¡å— 2: Reflection æ™ºèƒ½ä½“ ---
# 1. åˆå§‹æ‰§è¡Œæç¤ºè¯
INITIAL_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Pythonç¨‹åºå‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ï¼Œç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ã€‚
ä½ çš„ä»£ç å¿…é¡»åŒ…å«å®Œæ•´çš„å‡½æ•°ç­¾åã€æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå¹¶éµå¾ªPEP 8ç¼–ç è§„èŒƒã€‚

è¦æ±‚: {task}

è¯·ç›´æ¥è¾“å‡ºä»£ç ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""
# 2. åæ€æç¤ºè¯
REFLECT_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½æå…¶ä¸¥æ ¼çš„ä»£ç è¯„å®¡ä¸“å®¶å’Œèµ„æ·±ç®—æ³•å·¥ç¨‹å¸ˆï¼Œå¯¹ä»£ç çš„æ€§èƒ½æœ‰æè‡´çš„è¦æ±‚ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å®¡æŸ¥ä»¥ä¸‹Pythonä»£ç ï¼Œå¹¶ä¸“æ³¨äºæ‰¾å‡ºå…¶åœ¨**ç®—æ³•æ•ˆç‡**ä¸Šçš„ä¸»è¦ç“¶é¢ˆã€‚

# åŸå§‹ä»»åŠ¡:
{task}

# å¾…å®¡æŸ¥çš„ä»£ç :
```python
{code}
```

è¯·åˆ†æè¯¥ä»£ç çš„æ—¶é—´å¤æ‚åº¦ï¼Œå¹¶æ€è€ƒæ˜¯å¦å­˜åœ¨ä¸€ç§**ç®—æ³•ä¸Šæ›´ä¼˜**çš„è§£å†³æ–¹æ¡ˆæ¥æ˜¾è‘—æå‡æ€§èƒ½ã€‚
å¦‚æœå­˜åœ¨ï¼Œè¯·æ¸…æ™°åœ°æŒ‡å‡ºå½“å‰ç®—æ³•çš„ä¸è¶³ï¼Œå¹¶æå‡ºå…·ä½“çš„ã€å¯è¡Œçš„æ”¹è¿›ç®—æ³•å»ºè®®ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ç­›æ³•æ›¿ä»£è¯•é™¤æ³•ï¼‰ã€‚
å¦‚æœä»£ç åœ¨ç®—æ³•å±‚é¢å·²ç»è¾¾åˆ°æœ€ä¼˜ï¼Œæ‰èƒ½å›ç­”â€œæ— éœ€æ”¹è¿›â€ã€‚

è¯·ç›´æ¥è¾“å‡ºä½ çš„åé¦ˆï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""
# 3. ä¼˜åŒ–æç¤ºè¯
REFINE_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Pythonç¨‹åºå‘˜ã€‚ä½ æ­£åœ¨æ ¹æ®ä¸€ä½ä»£ç è¯„å®¡ä¸“å®¶çš„åé¦ˆæ¥ä¼˜åŒ–ä½ çš„ä»£ç ã€‚

# åŸå§‹ä»»åŠ¡:
{task}

# ä½ ä¸Šä¸€è½®å°è¯•çš„ä»£ç :
{last_code_attempt}

# è¯„å®¡å‘˜çš„åé¦ˆ:
{feedback}

è¯·æ ¹æ®è¯„å®¡å‘˜çš„åé¦ˆï¼Œç”Ÿæˆä¸€ä¸ªä¼˜åŒ–åçš„æ–°ç‰ˆæœ¬ä»£ç ã€‚
ä½ çš„ä»£ç å¿…é¡»åŒ…å«å®Œæ•´çš„å‡½æ•°ç­¾åã€æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå¹¶éµå¾ªPEP 8ç¼–ç è§„èŒƒã€‚
è¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„ä»£ç ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è§£é‡Šã€‚
"""

class ReflectionAgent:
    def __init__(self,llm_client:HelloLLMAgent,max_iterations:int=3):
        self.llm_client = llm_client
        self.max_iterations = max_iterations
        self.memory = Memory()

    def run_task(self,task:str):
        print(f"\n--- å¼€å§‹å¤„ç†ä»»åŠ¡ ---\nä»»åŠ¡: {task}")
        print("\n--- æ­£åœ¨è¿›è¡Œåˆå§‹å°è¯• ---")
        initial_prompt = INITIAL_PROMPT_TEMPLATE.format(task=task)
        initial_code = self._get_llm_response(initial_prompt)
        self.memory.add_record("execution",initial_code)

        # --- 2. è¿­ä»£å¾ªç¯ï¼šåæ€ä¸ä¼˜åŒ– ---
        for i in range(self.max_iterations):
            print(f"\n--- ç¬¬ {i+1}/{self.max_iterations} è½®è¿­ä»£ ---")
            print("\n-> æ­£åœ¨è¿›è¡Œåæ€...")
            last_code = self.memory.get_last_execution()
            reflect_prompt = REFLECT_PROMPT_TEMPLATE.format(task=task,code = last_code)
            feed_back = self._get_llm_response(reflect_prompt)
            self.memory.add_record("reflection",feed_back)
             # b. æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if "æ— éœ€æ”¹è¿›" in feed_back or "no need for improvement" in feed_back.lower():
                print("\nâœ… åæ€è®¤ä¸ºä»£ç å·²æ— éœ€æ”¹è¿›ï¼Œä»»åŠ¡å®Œæˆã€‚")
                break
            # c. ä¼˜åŒ–
            print("\n-> æ­£åœ¨è¿›è¡Œä¼˜åŒ–...")
            refine_prompt = REFINE_PROMPT_TEMPLATE.format(
                task=task,
                last_code_attempt=last_code,
                feedback=feed_back
            )
            refined_code = self._get_llm_response(refine_prompt)
            self.memory.add_record("execution", refined_code)
            final_code = self.memory.get_last_execution()
            print(f"\n--- ä»»åŠ¡å®Œæˆ ---\næœ€ç»ˆç”Ÿæˆçš„ä»£ç :\n{final_code}")
        return final_code

    def _get_llm_response(self,prompt:str)->str:
        """ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºè°ƒç”¨LLMå¹¶è·å–å®Œæ•´çš„æµå¼å“åº”ã€‚"""
        messages = [{"role": "user", "content": prompt}]
        response_text = self.llm_client.thinking(messages) or ""
        return response_text

if __name__ == '__main__':
    try:
        llm_client = HelloLLMAgent(
            model="qwen3:4b",
            baseUrl="http://localhost:11434/v1",
            apiKey="ollama"
        )
    except Exception as e:
        print(f"åˆå§‹åŒ–LLMå®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
        exit()

    agent = ReflectionAgent(llm_client,max_iterations=2)

    task = "ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œæ‰¾å‡º1åˆ°nä¹‹é—´æ‰€æœ‰çš„ç´ æ•° (prime numbers)ã€‚"
    agent.run_task(task)






    
    
